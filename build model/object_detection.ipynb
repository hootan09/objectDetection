{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cloud-annotations/google-colab-training/blob/master/object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TZSL793i7KuM"
   },
   "source": [
    "# Setup\n",
    "The only thing you **NEED** to change is `ANNOTATIONS_FOLDER`.\n",
    "\n",
    "You can easily create your own annotations for free at [Cloud Annotations](https://cloud.annotations.ai).\n",
    "\n",
    "### Steps\n",
    "1. Start a new project\n",
    "1. Choose **`localization`** as the project type\n",
    "1. Label your images and videos\n",
    "1. Export an annotation folder by clicking `File > Export as Create ML`\n",
    "1. Unzip the folder and upload it to Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hVPzEKoLuEHy"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Things to change:\n",
    "ANNOTATIONS_FOLDER = 'carplate-detection'\n",
    "NUM_TRAIN_STEPS = 500\n",
    "MODEL_TYPE = 'ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18'\n",
    "CONFIG_TYPE = 'ssd_mobilenet_v1_quantized_300x300_coco14_sync'\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "GOOGLE_DRIVE_MOUNT    = '/home/niki/samiAi'\n",
    "ANNOTATIONS_PATH      = os.path.join(GOOGLE_DRIVE_MOUNT, ANNOTATIONS_FOLDER)\n",
    "ANNOTATIONS_JSON_PATH = os.path.join(ANNOTATIONS_PATH, 'annotations.json')\n",
    "\n",
    "CHECKPOINT_PATH = '/home/niki/samiAi/checkpoint'\n",
    "OUTPUT_PATH     = '/home/niki/samiAi/output'\n",
    "EXPORTED_PATH   = '/home/niki/samiAi/exported'\n",
    "DATA_PATH       = '/home/niki/samiAi/data'\n",
    "\n",
    "LABEL_MAP_PATH    = os.path.join(DATA_PATH, 'label_map.pbtxt')\n",
    "TRAIN_RECORD_PATH = os.path.join(DATA_PATH, 'train.record')\n",
    "VAL_RECORD_PATH   = os.path.join(DATA_PATH, 'val.record')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XINCKkPshgz"
   },
   "source": [
    "# Install the TensorFlow Object Detection API\n",
    "In order to use the TensorFlow Object Detection API, we need to clone it's GitHub Repo.\n",
    "\n",
    "### Dependencies\n",
    "Most of the dependencies required come preloaded in Google Colab. The only additional package we need to install is TensorFlow.js, which is used for converting our trained model to a model that is compatible for the web.\n",
    "\n",
    "### Protocol Buffers\n",
    "The TensorFlow Object Detection API relies on what are called `protocol buffers` (also known as `protobufs`). Protobufs are a language neutral way to describe information. That means you can write a protobuf once and then compile it to be used with other languages, like Python, Java or C.\n",
    "\n",
    "The `protoc` command used below is compiling all the protocol buffers in the `object_detection/protos` folder for Python.\n",
    "\n",
    "### Environment\n",
    "To use the object detection api we need to add it to our `PYTHONPATH` along with `slim` which contains code for training and evaluating several widely used Convolutional Neural Network (CNN) image classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o33_jgwGm3NV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf_slim in /home/niki/anaconda3/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in /home/niki/anaconda3/lib/python3.7/site-packages (from tf_slim) (0.9.0)\n",
      "Requirement already satisfied: six in /home/niki/anaconda3/lib/python3.7/site-packages (from absl-py>=0.2.2->tf_slim) (1.11.0)\n",
      "Requirement already satisfied: tensorflowjs==1.4.0 in /home/niki/anaconda3/lib/python3.7/site-packages (1.4.0)\n",
      "/home/niki/samiAi/models/research\n",
      "object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used.\n"
     ]
    }
   ],
   "source": [
    "#%tensorflow_version 1.x\n",
    "import os\n",
    "\n",
    "!cd /home/niki/samiAi\n",
    "#!git clone --depth=1 https://github.com/tensorflow/models.git\n",
    "!pip install tf_slim\n",
    "!pip install --no-deps tensorflowjs==1.4.0\n",
    "\n",
    "%cd /home/niki/samiAi/models/research\n",
    "!protoc object_detection/protos/*.proto --python_out=.\n",
    "\n",
    "pwd = os.getcwd()\n",
    "os.environ['PYTHONPATH'] = '/home/niki/anaconda3/bin/python'\n",
    "os.environ['PYTHONPATH'] += f':{pwd}:{pwd}/slim'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wS1ZDbJ660Wv"
   },
   "source": [
    "# Test the setup\n",
    "If everything was set up properly and nothing went wrong, we should be able to run this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iM8sOHwL64Rp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests under Python 3.7.6: /home/niki/anaconda3/bin/python\n",
      "[ RUN      ] ModelBuilderTF1Test.test_create_context_rcnn_from_config_with_params(True)\n",
      "/home/niki/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/data_structures.py:669: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  if not isinstance(wrapped_dict, collections.Mapping):\n",
      "[       OK ] ModelBuilderTF1Test.test_create_context_rcnn_from_config_with_params(True)\n",
      "[ RUN      ] ModelBuilderTF1Test.test_create_context_rcnn_from_config_with_params(False)\n",
      "[       OK ] ModelBuilderTF1Test.test_create_context_rcnn_from_config_with_params(False)\n",
      "[ RUN      ] ModelBuilderTF1Test.test_create_experimental_model\n",
      "[       OK ] ModelBuilderTF1Test.test_create_experimental_model\n",
      "[ RUN      ] ModelBuilderTF1Test.test_create_faster_rcnn_from_config_with_crop_feature(True)\n",
      "[       OK ] ModelBuilderTF1Test.test_create_faster_rcnn_from_config_with_crop_feature(True)\n",
      "[ RUN      ] ModelBuilderTF1Test.test_create_faster_rcnn_from_config_with_crop_feature(False)\n",
      "[       OK ] ModelBuilderTF1Test.test_create_faster_rcnn_from_config_with_crop_feature(False)\n",
      "[ RUN      ] ModelBuilderTF1Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "[       OK ] ModelBuilderTF1Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "[ RUN      ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "[       OK ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "[       OK ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "[       OK ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "[       OK ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF1Test.test_create_rfcn_model_from_config\n",
      "[       OK ] ModelBuilderTF1Test.test_create_rfcn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF1Test.test_create_ssd_fpn_model_from_config\n",
      "[       OK ] ModelBuilderTF1Test.test_create_ssd_fpn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF1Test.test_create_ssd_models_from_config\n",
      "[       OK ] ModelBuilderTF1Test.test_create_ssd_models_from_config\n",
      "[ RUN      ] ModelBuilderTF1Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "[       OK ] ModelBuilderTF1Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "[ RUN      ] ModelBuilderTF1Test.test_invalid_first_stage_nms_iou_threshold\n",
      "[       OK ] ModelBuilderTF1Test.test_invalid_first_stage_nms_iou_threshold\n",
      "[ RUN      ] ModelBuilderTF1Test.test_invalid_model_config_proto\n",
      "[       OK ] ModelBuilderTF1Test.test_invalid_model_config_proto\n",
      "[ RUN      ] ModelBuilderTF1Test.test_invalid_second_stage_batch_size\n",
      "[       OK ] ModelBuilderTF1Test.test_invalid_second_stage_batch_size\n",
      "[ RUN      ] ModelBuilderTF1Test.test_session\n",
      "[  SKIPPED ] ModelBuilderTF1Test.test_session\n",
      "[ RUN      ] ModelBuilderTF1Test.test_unknown_faster_rcnn_feature_extractor\n",
      "[       OK ] ModelBuilderTF1Test.test_unknown_faster_rcnn_feature_extractor\n",
      "[ RUN      ] ModelBuilderTF1Test.test_unknown_meta_architecture\n",
      "[       OK ] ModelBuilderTF1Test.test_unknown_meta_architecture\n",
      "[ RUN      ] ModelBuilderTF1Test.test_unknown_ssd_feature_extractor\n",
      "[       OK ] ModelBuilderTF1Test.test_unknown_ssd_feature_extractor\n",
      "----------------------------------------------------------------------\n",
      "Ran 21 tests in 0.789s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "!python object_detection/builders/model_builder_tf1_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ISX8k0TfdDHj"
   },
   "source": [
    "# Generate a Label Map\n",
    "One piece of data the Object Detection API needs is a label map protobuf. The label map associates an integer id to the text representation of the label. The ids are indexed by 1, meaning the first label will have an id of 1 not 0.\n",
    "\n",
    "Here is an example of what a label map looks like:\n",
    "```\n",
    "item {\n",
    "  id: 1\n",
    "  name: 'Cat'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 2\n",
    "  name: 'Dog'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 3\n",
    "  name: 'Gold Fish'\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJsKCG3UdDsn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Get a list of labels from the annotations.json\n",
    "labels = {}\n",
    "with open(ANNOTATIONS_JSON_PATH) as f:\n",
    "  annotations = json.load(f)\n",
    "  labels = {l['label'] for a in annotations for l in a['annotations']}\n",
    "\n",
    "# Create a file named label_map.pbtxt\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "with open(LABEL_MAP_PATH, 'w') as f:\n",
    "  # Loop through all of the labels and write each label to the file with an id\n",
    "  for idx, label in enumerate(labels):\n",
    "    f.write('item {\\n')\n",
    "    f.write(\"\\tname: '{}'\\n\".format(label))\n",
    "    f.write('\\tid: {}\\n'.format(idx + 1)) # indexes must start at 1\n",
    "    f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "siRNKiuvsz25"
   },
   "source": [
    "# Generate TFRecords\n",
    "The TensorFlow Object Detection API expects our data to be in the format of TFRecords.\n",
    "\n",
    "The TFRecord format is a collection of serialized feature dicts, one for each image, looking something like this:\n",
    "```\n",
    "{\n",
    "  'image/height': 1800,\n",
    "  'image/width': 2400,\n",
    "  'image/filename': 'image1.jpg',\n",
    "  'image/source_id': 'image1.jpg',\n",
    "  'image/encoded': ACTUAL_ENCODED_IMAGE_DATA_AS_BYTES,\n",
    "  'image/format': 'jpeg',\n",
    "  'image/object/bbox/xmin': [0.7255949630314233, 0.8845598428835489],\n",
    "  'image/object/bbox/xmax': [0.9695875693160814, 1.0000000000000000],\n",
    "  'image/object/bbox/ymin': [0.5820120073891626, 0.1829972290640394],\n",
    "  'image/object/bbox/ymax': [1.0000000000000000, 0.9662484605911330],\n",
    "  'image/object/class/text': (['Cat', 'Dog']),\n",
    "  'image/object/class/label': ([1, 2])\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAkOvP-gZR1x"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import random\n",
    "\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "def create_tf_record(annotations, label_map, image_path, output):\n",
    "  # Create a train.record TFRecord file.\n",
    "  with tf.python_io.TFRecordWriter(output) as writer:\n",
    "    # Loop through all the training examples.\n",
    "    for annotation in annotations:\n",
    "      try:\n",
    "        # Make sure the image is actually a file\n",
    "        img_path = os.path.join(image_path, annotation['image'])   \n",
    "        if not os.path.isfile(img_path):\n",
    "          continue\n",
    "          \n",
    "        # Read in the image.\n",
    "        with tf.gfile.GFile(img_path, 'rb') as fid:\n",
    "          encoded_jpg = fid.read()\n",
    "\n",
    "        # Open the image with PIL so we can check that it's a jpeg and get the image\n",
    "        # dimensions.\n",
    "        encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "        image = PIL.Image.open(encoded_jpg_io)\n",
    "        if image.format != 'JPEG':\n",
    "          raise ValueError('Image format not JPEG')\n",
    "\n",
    "        width, height = image.size\n",
    "\n",
    "        # Initialize all the arrays.\n",
    "        xmins = []\n",
    "        xmaxs = []\n",
    "        ymins = []\n",
    "        ymaxs = []\n",
    "        classes_text = []\n",
    "        classes = []\n",
    "\n",
    "        # The class text is the label name and the class is the id. If there are 3\n",
    "        # cats in the image and 1 dog, it may look something like this:\n",
    "        # classes_text = ['Cat', 'Cat', 'Dog', 'Cat']\n",
    "        # classes      = [  1  ,   1  ,   2  ,   1  ]\n",
    "\n",
    "        # For each image, loop through all the annotations and append their values.\n",
    "        for a in annotation['annotations']:\n",
    "          coord = a['coordinates']\n",
    "          xmin = coord['x'] - (coord['width'] / 2)\n",
    "          xmax = xmin + coord['width']\n",
    "          ymin = coord['y'] - (coord['height'] / 2)\n",
    "          ymax = ymin + coord['height']\n",
    "          xmins.append(max(xmin / width, 0))\n",
    "          xmaxs.append(min(xmax / width, 1))\n",
    "          ymins.append(max(ymin / height, 0))\n",
    "          ymaxs.append(min(ymax / height, 1))\n",
    "          label = a['label']\n",
    "          classes_text.append(label.encode('utf8'))\n",
    "          classes.append(label_map[label])\n",
    "      \n",
    "        # Create the TFExample.\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "          'image/height': dataset_util.int64_feature(height),\n",
    "          'image/width': dataset_util.int64_feature(width),\n",
    "          'image/filename': dataset_util.bytes_feature(annotation['image'].encode('utf8')),\n",
    "          'image/source_id': dataset_util.bytes_feature(annotation['image'].encode('utf8')),\n",
    "          'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "          'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "          'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "          'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "          'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "          'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "          'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "          'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "        }))\n",
    "        if tf_example:\n",
    "          # Write the TFExample to the TFRecord.\n",
    "          writer.write(tf_example.SerializeToString())\n",
    "      except ValueError:\n",
    "        print('Invalid example, ignoring.')\n",
    "        pass\n",
    "      except IOError:\n",
    "        # print(\"Can't read example, ignoring.\")\n",
    "        pass\n",
    "\n",
    "with open(ANNOTATIONS_JSON_PATH) as f:\n",
    "  annotations = json.load(f)\n",
    "  # Load the label map we created.\n",
    "  label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
    "\n",
    "  random.seed(42)\n",
    "  random.shuffle(annotations)\n",
    "  num_train = int(0.7 * len(annotations))\n",
    "  train_examples = annotations[:num_train]\n",
    "  val_examples = annotations[num_train:]\n",
    "\n",
    "  create_tf_record(train_examples, label_map, ANNOTATIONS_PATH, TRAIN_RECORD_PATH)\n",
    "  create_tf_record(val_examples, label_map, ANNOTATIONS_PATH, VAL_RECORD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6DhYpAS7gX2"
   },
   "source": [
    "# Download a base model\n",
    "Training a model from scratch can take days and tons of data. We can mitigate this by using a pretrained model checkpoint. Instead of starting from nothing, we can add to what was already learned with our own data.\n",
    "\n",
    "There are several pretrained model checkpoints that can be downloaded from the [model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).\n",
    "\n",
    "The model we will be training is the SSD MobileNet architecture. SSD MobileNet models have a very small file size and can execute very quickly, compromising little accuracy, which makes it perfect for running in the browser. Additionally, we will be using `quantization`. When we say the model is `quantized` it means instead of using `float32` as the datatype of our numbers we are using `float16` or `int8`.\n",
    "\n",
    "- `float32(PI)` = `3.1415927` 32 bits\n",
    "- `float16(PI)` = `3.14` 16 bits\n",
    "- `int8(PI)` = `3` 8 bits\n",
    "\n",
    "We do this because it can cut our model size down by around a factor of 4! An unquantized version of SSD MobileNet that I trained was `22.3 MB`, but the quantized version was `5.7 MB` that's a `~75%` reduction ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oHD1Jm0v7jfz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "import six.moves.urllib as urllib\n",
    "\n",
    "download_base = 'http://download.tensorflow.org/models/object_detection/'\n",
    "model = MODEL_TYPE + '.tar.gz'\n",
    "tmp = '/home/niki/samiAi/checkpoint.tar.gz'\n",
    "\n",
    "if not (os.path.exists(CHECKPOINT_PATH)):\n",
    "  # Download the checkpoint\n",
    "  opener = urllib.request.URLopener()\n",
    "  opener.retrieve(download_base + model, tmp)\n",
    "\n",
    "  # Extract all the `model.ckpt` files.\n",
    "  with tarfile.open(tmp) as tar:\n",
    "    for member in tar.getmembers():\n",
    "      member.name = os.path.basename(member.name)\n",
    "      if 'model.ckpt' in member.name:\n",
    "        tar.extract(member, path=CHECKPOINT_PATH)\n",
    "\n",
    "  #os.remove(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXlvFvwUHrui"
   },
   "source": [
    "# Model config\n",
    "The final thing we need to do is inject our pipline with the amount of labels we have and where to find the label map, TFRecord and model checkpoint. We also need to change the the batch size, because the default batch size of 128 is too large for Colab to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8CVExv6HsJS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Maybe overwriting model.ssd.num_classes: 1\n",
      "INFO:tensorflow:Maybe overwriting train_config.batch_size: 24\n",
      "INFO:tensorflow:Maybe overwriting train_input_path: /home/niki/samiAi/data/train.record\n",
      "INFO:tensorflow:Maybe overwriting eval_input_path: /home/niki/samiAi/data/val.record\n",
      "INFO:tensorflow:Maybe overwriting train_config.fine_tune_checkpoint: /home/niki/samiAi/checkpoint/model.ckpt\n",
      "INFO:tensorflow:Maybe overwriting label_map_path: /home/niki/samiAi/data/label_map.pbtxt\n",
      "INFO:tensorflow:Writing pipeline config file to /home/niki/samiAi/data/pipeline.config\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from google.protobuf import text_format\n",
    "\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "pipeline_skeleton = '/home/niki/samiAi/models/research/object_detection/samples/configs/' + CONFIG_TYPE + '.config'\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_skeleton)\n",
    "\n",
    "label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
    "num_classes = len(label_map.keys())\n",
    "meta_arch = configs[\"model\"].WhichOneof(\"model\")\n",
    "\n",
    "override_dict = {\n",
    "  'model.{}.num_classes'.format(meta_arch): num_classes,\n",
    "  'train_config.batch_size': 24,\n",
    "  'train_input_path': TRAIN_RECORD_PATH,\n",
    "  'eval_input_path': VAL_RECORD_PATH,\n",
    "  'train_config.fine_tune_checkpoint': os.path.join(CHECKPOINT_PATH, 'model.ckpt'),\n",
    "  'label_map_path': LABEL_MAP_PATH\n",
    "}\n",
    "\n",
    "configs = config_util.merge_external_params_with_configs(configs, kwargs_dict=override_dict)\n",
    "pipeline_config = config_util.create_pipeline_proto_from_configs(configs)\n",
    "config_util.save_pipeline_config(pipeline_config, DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNYIZK1xVNAa"
   },
   "source": [
    "# Start training\n",
    "We can start a training run by calling the `model_main` script, passing:\n",
    "- The location of the `pipepline.config` we created\n",
    "- Where we want to save the model\n",
    "- How many steps we want to train the model (the longer you train, the more potential there is to learn)\n",
    "- The number of evaluation steps (or how often to test the model) gives us an idea of how well the model is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wv5h2bwBVO0V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/niki/samiAi/models/research\n",
      "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
      "W0716 15:55:38.185163 139790369810240 model_lib.py:758] Forced number of epochs for all eval validations to be 1.\n",
      "INFO:tensorflow:Maybe overwriting train_steps: 500\n",
      "I0716 15:55:38.185587 139790369810240 config_util.py:552] Maybe overwriting train_steps: 500\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I0716 15:55:38.185840 139790369810240 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n",
      "I0716 15:55:38.186046 139790369810240 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: 1\n",
      "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
      "I0716 15:55:38.186280 139790369810240 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n",
      "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
      "W0716 15:55:38.186614 139790369810240 model_lib.py:774] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
      "INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu None\n",
      "I0716 15:55:38.186851 139790369810240 model_lib.py:809] create_estimator_and_inputs: use_tpu False, export_to_tpu None\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/niki/samiAi/output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f231a355310>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I0716 15:55:38.188293 139790369810240 estimator.py:212] Using config: {'_model_dir': '/home/niki/samiAi/output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f231a355310>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f231a33ec20>) includes params argument, but params are not passed to Estimator.\n",
      "W0716 15:55:38.189228 139790369810240 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f231a33ec20>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "I0716 15:55:38.191407 139790369810240 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "I0716 15:55:38.192053 139790369810240 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "I0716 15:55:38.192807 139790369810240 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "WARNING:tensorflow:From /home/niki/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0716 15:55:38.205966 139790369810240 deprecation.py:323] From /home/niki/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W0716 15:55:38.352037 139790369810240 dataset_builder.py:83] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /home/niki/samiAi/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "W0716 15:55:38.370467 139790369810240 deprecation.py:323] From /home/niki/samiAi/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "WARNING:tensorflow:From /home/niki/samiAi/models/research/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W0716 15:55:38.617581 139790369810240 deprecation.py:323] From /home/niki/samiAi/models/research/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /home/niki/samiAi/models/research/object_detection/inputs.py:77: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W0716 15:56:34.579563 139790369810240 deprecation.py:323] From /home/niki/samiAi/models/research/object_detection/inputs.py:77: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /home/niki/samiAi/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0716 15:56:35.018781 139790369810240 deprecation.py:323] From /home/niki/samiAi/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/niki/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:1004: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "W0716 15:57:01.909527 139790369810240 api.py:332] From /home/niki/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:1004: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From /home/niki/samiAi/models/research/object_detection/inputs.py:259: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0716 15:57:15.351624 139790369810240 deprecation.py:323] From /home/niki/samiAi/models/research/object_detection/inputs.py:259: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I0716 15:57:28.985538 139790369810240 estimator.py:1148] Calling model_fn.\n",
      "WARNING:tensorflow:From /home/niki/anaconda3/lib/python3.7/site-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "W0716 15:57:29.023648 139790369810240 deprecation.py:323] From /home/niki/anaconda3/lib/python3.7/site-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0716 15:57:34.154126 139790369810240 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0716 15:57:34.273272 139790369810240 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0716 15:57:34.375516 139790369810240 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0716 15:57:34.492668 139790369810240 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0716 15:57:34.864217 139790369810240 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0716 15:57:34.977311 139790369810240 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/add_fold\n",
      "I0716 15:57:58.792304 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/add_fold\n",
      "I0716 15:57:58.793739 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/add_fold\n",
      "I0716 15:57:58.795017 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/add_fold\n",
      "I0716 15:57:58.796215 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/add_fold\n",
      "I0716 15:57:58.797393 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/add_fold\n",
      "I0716 15:57:58.798711 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/add_fold\n",
      "I0716 15:57:58.800080 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/add_fold\n",
      "I0716 15:57:58.801299 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/add_fold\n",
      "I0716 15:57:58.802528 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/add_fold\n",
      "I0716 15:57:58.803757 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/add_fold\n",
      "I0716 15:57:58.804891 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/add_fold\n",
      "I0716 15:57:58.806056 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/add_fold\n",
      "I0716 15:57:58.807198 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/add_fold\n",
      "I0716 15:57:58.808655 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/add_fold\n",
      "I0716 15:57:58.809803 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/add_fold\n",
      "I0716 15:57:58.811061 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/add_fold\n",
      "I0716 15:57:58.811758 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/add_fold\n",
      "I0716 15:57:58.812783 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/add_fold\n",
      "I0716 15:57:58.814088 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/add_fold\n",
      "I0716 15:57:58.815352 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/add_fold\n",
      "I0716 15:57:58.816488 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/add_fold\n",
      "I0716 15:57:58.818039 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/add_fold\n",
      "I0716 15:57:58.819855 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/add_fold\n",
      "I0716 15:57:58.821702 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/add_fold\n",
      "I0716 15:57:58.823491 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/add_fold\n",
      "I0716 15:57:58.825586 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/add_fold\n",
      "I0716 15:57:58.827376 139790369810240 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/add_fold\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "I0716 15:58:20.417093 139790369810240 estimator.py:1150] Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "I0716 15:58:20.420173 139790369810240 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0716 15:58:35.849459 139790369810240 monitored_session.py:240] Graph was finalized.\n",
      "2020-07-16 15:58:36.262644: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2294795000 Hz\n",
      "2020-07-16 15:58:36.275813: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5648a12614c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-16 15:58:36.275985: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-07-16 15:58:36.314283: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2020-07-16 15:58:36.314425: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-07-16 15:58:36.314488: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (niki-VirtualBox): /proc/driver/nvidia/version does not exist\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0716 15:58:51.334297 139790369810240 session_manager.py:500] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0716 15:58:52.973888 139790369810240 session_manager.py:502] Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/niki/samiAi/output/model.ckpt.\n",
      "I0716 15:59:42.531064 139790369810240 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /home/niki/samiAi/output/model.ckpt.\n",
      "2020-07-16 16:00:15.211925: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 17743872 exceeds 10% of system memory.\n",
      "2020-07-16 16:00:15.310682: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 17743872 exceeds 10% of system memory.\n",
      "2020-07-16 16:00:15.525934: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 17743872 exceeds 10% of system memory.\n",
      "2020-07-16 16:00:15.635644: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 17743872 exceeds 10% of system memory.\n",
      "2020-07-16 16:00:15.745441: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 17743872 exceeds 10% of system memory.\n",
      "Killed\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!rm -rf $OUTPUT_PATH\n",
    "!python -m object_detection.model_main \\\n",
    "    --pipeline_config_path=$DATA_PATH/pipeline.config \\\n",
    "    --model_dir=$OUTPUT_PATH \\\n",
    "    --num_train_steps=$NUM_TRAIN_STEPS \\\n",
    "    --num_eval_steps=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwNtvgtdoB-C"
   },
   "source": [
    "# Export inference graph\n",
    "After your model has been trained, you might have a few checkpoints available. A checkpoint is usually emitted every 500 training steps. Each checkpoint is a snapshot of your model at that point in training. In the event that a long running training process crashes, you can pick up at the last checkpoint instead of starting from scratch.\n",
    "\n",
    "We need to export a checkpoint to a TensorFlow graph proto in order to actually use it. We use regex to find the checkpoint with the highest training step and export it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZgP_FZUoE0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/niki/samiAi/output/model.ckpt-0\n",
      "WARNING:tensorflow:From /home/niki/anaconda3/lib/python3.7/site-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "W0716 16:11:58.150952 140481695352640 deprecation.py:323] From /home/niki/anaconda3/lib/python3.7/site-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0716 16:12:04.380306 140481695352640 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0716 16:12:04.562045 140481695352640 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0716 16:12:04.744422 140481695352640 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0716 16:12:04.909498 140481695352640 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0716 16:12:05.089919 140481695352640 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0716 16:12:05.274695 140481695352640 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "WARNING:tensorflow:From /home/niki/samiAi/models/research/object_detection/core/post_processing.py:583: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0716 16:12:06.236724 140481695352640 deprecation.py:323] From /home/niki/samiAi/models/research/object_detection/core/post_processing.py:583: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/niki/samiAi/models/research/object_detection/exporter.py:474: get_or_create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "W0716 16:12:07.619586 140481695352640 deprecation.py:323] From /home/niki/samiAi/models/research/object_detection/exporter.py:474: get_or_create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/add_fold\n",
      "I0716 16:12:10.889913 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/add_fold\n",
      "I0716 16:12:10.890645 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/add_fold\n",
      "I0716 16:12:10.891236 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/add_fold\n",
      "I0716 16:12:10.891882 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/add_fold\n",
      "I0716 16:12:10.892458 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/add_fold\n",
      "I0716 16:12:10.893022 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/add_fold\n",
      "I0716 16:12:10.893558 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/add_fold\n",
      "I0716 16:12:10.894092 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/add_fold\n",
      "I0716 16:12:10.894675 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/add_fold\n",
      "I0716 16:12:10.895382 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/add_fold\n",
      "I0716 16:12:10.895925 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/add_fold\n",
      "I0716 16:12:10.896399 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/add_fold\n",
      "I0716 16:12:10.896745 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/add_fold\n",
      "I0716 16:12:10.897069 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/add_fold\n",
      "I0716 16:12:10.897616 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/add_fold\n",
      "I0716 16:12:10.898237 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/add_fold\n",
      "I0716 16:12:10.898761 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/add_fold\n",
      "I0716 16:12:10.899395 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/add_fold\n",
      "I0716 16:12:10.899915 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/add_fold\n",
      "I0716 16:12:10.900498 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/add_fold\n",
      "I0716 16:12:10.901054 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/add_fold\n",
      "I0716 16:12:10.901604 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/add_fold\n",
      "I0716 16:12:10.902158 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/add_fold\n",
      "I0716 16:12:10.902767 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/add_fold\n",
      "I0716 16:12:10.903318 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/add_fold\n",
      "I0716 16:12:10.903865 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/add_fold\n",
      "I0716 16:12:10.904407 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/add_fold\n",
      "I0716 16:12:10.904967 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/add_fold\n",
      "I0716 16:12:10.905491 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/add_fold\n",
      "I0716 16:12:10.906045 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/add_fold\n",
      "I0716 16:12:10.906646 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/add_fold\n",
      "I0716 16:12:10.907271 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/add_fold\n",
      "I0716 16:12:10.907793 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/add_fold\n",
      "I0716 16:12:10.908337 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/add_fold\n",
      "I0716 16:12:10.908943 140481695352640 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/add_fold\n",
      "WARNING:tensorflow:From /home/niki/samiAi/models/research/object_detection/exporter.py:653: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
      "Instructions for updating:\n",
      "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
      "W0716 16:12:10.915605 140481695352640 deprecation.py:323] From /home/niki/samiAi/models/research/object_detection/exporter.py:653: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
      "Instructions for updating:\n",
      "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
      "WARNING:tensorflow:From /home/niki/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
      "W0716 16:12:10.916861 140481695352640 deprecation.py:323] From /home/niki/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 ops no flops stats due to incomplete shapes.\n",
      "Parsing Inputs...\n",
      "Incomplete shape.\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              0\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   name\n",
      "-account_type_regexes       _trainable_variables\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          .*BatchNorm.*\n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     params\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "Incomplete shape.\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "param: Number of parameters (in the Variable).\n",
      "\n",
      "Profile:\n",
      "node name | # parameters\n",
      "_TFProfRoot (--/5.49m params)\n",
      "  BoxPredictor_0 (--/9.23k params)\n",
      "    BoxPredictor_0/BoxEncodingPredictor (--/6.16k params)\n",
      "      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n",
      "      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x512x12, 6.14k/6.14k params)\n",
      "    BoxPredictor_0/ClassPredictor (--/3.08k params)\n",
      "      BoxPredictor_0/ClassPredictor/biases (6, 6/6 params)\n",
      "      BoxPredictor_0/ClassPredictor/weights (1x1x512x6, 3.07k/3.07k params)\n",
      "  BoxPredictor_1 (--/36.90k params)\n",
      "    BoxPredictor_1/BoxEncodingPredictor (--/24.60k params)\n",
      "      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n",
      "      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x1024x24, 24.58k/24.58k params)\n",
      "    BoxPredictor_1/ClassPredictor (--/12.30k params)\n",
      "      BoxPredictor_1/ClassPredictor/biases (12, 12/12 params)\n",
      "      BoxPredictor_1/ClassPredictor/weights (1x1x1024x12, 12.29k/12.29k params)\n",
      "  BoxPredictor_2 (--/18.47k params)\n",
      "    BoxPredictor_2/BoxEncodingPredictor (--/12.31k params)\n",
      "      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n",
      "      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n",
      "    BoxPredictor_2/ClassPredictor (--/6.16k params)\n",
      "      BoxPredictor_2/ClassPredictor/biases (12, 12/12 params)\n",
      "      BoxPredictor_2/ClassPredictor/weights (1x1x512x12, 6.14k/6.14k params)\n",
      "  BoxPredictor_3 (--/9.25k params)\n",
      "    BoxPredictor_3/BoxEncodingPredictor (--/6.17k params)\n",
      "      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n",
      "      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
      "    BoxPredictor_3/ClassPredictor (--/3.08k params)\n",
      "      BoxPredictor_3/ClassPredictor/biases (12, 12/12 params)\n",
      "      BoxPredictor_3/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
      "  BoxPredictor_4 (--/9.25k params)\n",
      "    BoxPredictor_4/BoxEncodingPredictor (--/6.17k params)\n",
      "      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n",
      "      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
      "    BoxPredictor_4/ClassPredictor (--/3.08k params)\n",
      "      BoxPredictor_4/ClassPredictor/biases (12, 12/12 params)\n",
      "      BoxPredictor_4/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
      "  BoxPredictor_5 (--/4.64k params)\n",
      "    BoxPredictor_5/BoxEncodingPredictor (--/3.10k params)\n",
      "      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n",
      "      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x128x24, 3.07k/3.07k params)\n",
      "    BoxPredictor_5/ClassPredictor (--/1.55k params)\n",
      "      BoxPredictor_5/ClassPredictor/biases (12, 12/12 params)\n",
      "      BoxPredictor_5/ClassPredictor/weights (1x1x128x12, 1.54k/1.54k params)\n",
      "  FeatureExtractor (--/5.41m params)\n",
      "    FeatureExtractor/MobilenetV1 (--/5.41m params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_0 (--/864 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_0/weights (3x3x3x32, 864/864 params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_10_depthwise (--/4.61k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_10_pointwise (--/262.14k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/weights (1x1x512x512, 262.14k/262.14k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_11_depthwise (--/4.61k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_11_pointwise (--/262.14k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/weights (1x1x512x512, 262.14k/262.14k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_12_depthwise (--/4.61k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_12_pointwise (--/524.29k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/weights (1x1x512x1024, 524.29k/524.29k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_13_depthwise (--/9.22k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/depthwise_weights (3x3x1024x1, 9.22k/9.22k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise (--/1.05m params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/weights (1x1x1024x1024, 1.05m/1.05m params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256 (--/262.14k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/weights (1x1x1024x256, 262.14k/262.14k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128 (--/65.54k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128 (--/32.77k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64 (--/16.38k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512 (--/1.18m params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/weights (3x3x256x512, 1.18m/1.18m params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256 (--/294.91k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256 (--/294.91k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128 (--/73.73k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/weights (3x3x64x128, 73.73k/73.73k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_1_depthwise (--/288 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/depthwise_weights (3x3x32x1, 288/288 params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_1_pointwise (--/2.05k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/weights (1x1x32x64, 2.05k/2.05k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_2_depthwise (--/576 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/depthwise_weights (3x3x64x1, 576/576 params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_2_pointwise (--/8.19k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/weights (1x1x64x128, 8.19k/8.19k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_3_depthwise (--/1.15k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_3_pointwise (--/16.38k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/weights (1x1x128x128, 16.38k/16.38k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_4_depthwise (--/1.15k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_4_pointwise (--/32.77k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/weights (1x1x128x256, 32.77k/32.77k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_5_depthwise (--/2.30k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_5_pointwise (--/65.54k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/weights (1x1x256x256, 65.54k/65.54k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_6_depthwise (--/2.30k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_6_pointwise (--/131.07k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights (1x1x256x512, 131.07k/131.07k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_7_depthwise (--/4.61k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_7_pointwise (--/262.14k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights (1x1x512x512, 262.14k/262.14k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_8_depthwise (--/4.61k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_8_pointwise (--/262.14k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights (1x1x512x512, 262.14k/262.14k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_9_depthwise (--/4.61k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
      "      FeatureExtractor/MobilenetV1/Conv2d_9_pointwise (--/262.14k params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights (1x1x512x512, 262.14k/262.14k params)\n",
      "\n",
      "======================End of Report==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 ops no flops stats due to incomplete shapes.\n",
      "Parsing Inputs...\n",
      "Incomplete shape.\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "Incomplete shape.\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/5.42m flops)\n",
      "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/mul_fold (1.18m/1.18m flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/mul_fold (1.05m/1.05m flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/mul_fold (524.29k/524.29k flops)\n",
      "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/mul_fold (294.91k/294.91k flops)\n",
      "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/mul_fold (294.91k/294.91k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/mul_fold (262.14k/262.14k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/mul_fold (262.14k/262.14k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/mul_fold (262.14k/262.14k flops)\n",
      "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/mul_fold (262.14k/262.14k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/mul_fold (262.14k/262.14k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/mul_fold (262.14k/262.14k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/mul_fold (131.07k/131.07k flops)\n",
      "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/mul_fold (73.73k/73.73k flops)\n",
      "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/mul_fold (65.54k/65.54k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/mul_fold (65.54k/65.54k flops)\n",
      "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/mul_fold (32.77k/32.77k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/mul_fold (32.77k/32.77k flops)\n",
      "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/mul_fold (16.38k/16.38k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/mul_fold (16.38k/16.38k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/mul_fold (9.22k/9.22k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/mul_fold (8.19k/8.19k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/mul_fold (4.61k/4.61k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/mul_fold (4.61k/4.61k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/mul_fold (4.61k/4.61k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/mul_fold (4.61k/4.61k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/mul_fold (4.61k/4.61k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/mul_fold (4.61k/4.61k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/mul_fold (2.30k/2.30k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/mul_fold (2.30k/2.30k flops)\n",
      "  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)\n",
      "  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)\n",
      "  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/mul_fold (2.05k/2.05k flops)\n",
      "  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)\n",
      "  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)\n",
      "  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/mul_fold (1.15k/1.15k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/mul_fold (1.15k/1.15k flops)\n",
      "  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/mul_fold (864/864 flops)\n",
      "  MultipleGridAnchorGenerator/mul_29 (600/600 flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/mul_fold (576/576 flops)\n",
      "  MultipleGridAnchorGenerator/sub_2 (300/300 flops)\n",
      "  MultipleGridAnchorGenerator/mul_35 (300/300 flops)\n",
      "  MultipleGridAnchorGenerator/mul_36 (300/300 flops)\n",
      "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/mul_fold (288/288 flops)\n",
      "  MultipleGridAnchorGenerator/mul_37 (150/150 flops)\n",
      "  MultipleGridAnchorGenerator/sub_3 (108/108 flops)\n",
      "  MultipleGridAnchorGenerator/mul_43 (108/108 flops)\n",
      "  MultipleGridAnchorGenerator/mul_44 (108/108 flops)\n",
      "  MultipleGridAnchorGenerator/mul_45 (54/54 flops)\n",
      "  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n",
      "  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n",
      "  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n",
      "  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n",
      "  MultipleGridAnchorGenerator/mul_17 (19/19 flops)\n",
      "  MultipleGridAnchorGenerator/mul_18 (19/19 flops)\n",
      "  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n",
      "  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n",
      "  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n",
      "  MultipleGridAnchorGenerator/mul_25 (10/10 flops)\n",
      "  MultipleGridAnchorGenerator/mul_26 (10/10 flops)\n",
      "  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_33 (5/5 flops)\n",
      "  MultipleGridAnchorGenerator/mul_34 (5/5 flops)\n",
      "  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)\n",
      "  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n",
      "  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n",
      "  MultipleGridAnchorGenerator/mul_42 (3/3 flops)\n",
      "  MultipleGridAnchorGenerator/mul_41 (3/3 flops)\n",
      "  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n",
      "  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
      "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n",
      "  Preprocessor/map/while/Less (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/assert_equal_1/Equal (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)\n",
      "\n",
      "======================End of Report==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-16 16:12:24.583464: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2020-07-16 16:12:24.594757: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-07-16 16:12:24.595102: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (niki-VirtualBox): /proc/driver/nvidia/version does not exist\n",
      "2020-07-16 16:12:24.896540: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2294795000 Hz\n",
      "2020-07-16 16:12:24.900239: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55eab72e5ec0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-16 16:12:24.900584: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "INFO:tensorflow:Restoring parameters from /home/niki/samiAi/output/model.ckpt-0\n",
      "I0716 16:12:24.929923 140481695352640 saver.py:1284] Restoring parameters from /home/niki/samiAi/output/model.ckpt-0\n",
      "WARNING:tensorflow:From /home/niki/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W0716 16:12:35.540084 140481695352640 deprecation.py:323] From /home/niki/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /home/niki/samiAi/output/model.ckpt-0\n",
      "I0716 16:12:39.037570 140481695352640 saver.py:1284] Restoring parameters from /home/niki/samiAi/output/model.ckpt-0\n",
      "WARNING:tensorflow:From /home/niki/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "W0716 16:12:42.503169 140481695352640 deprecation.py:323] From /home/niki/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /home/niki/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "W0716 16:12:42.503566 140481695352640 deprecation.py:323] From /home/niki/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 387 variables.\n",
      "I0716 16:12:44.357535 140481695352640 graph_util_impl.py:334] Froze 387 variables.\n",
      "INFO:tensorflow:Converted 387 variables to const ops.\n",
      "I0716 16:12:44.683280 140481695352640 graph_util_impl.py:394] Converted 387 variables to const ops.\n",
      "WARNING:tensorflow:From /home/niki/samiAi/models/research/object_detection/exporter.py:384: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "W0716 16:12:47.515062 140481695352640 deprecation.py:323] From /home/niki/samiAi/models/research/object_detection/exporter.py:384: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:No assets to save.\n",
      "I0716 16:12:47.518125 140481695352640 builder_impl.py:640] No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "I0716 16:12:47.518643 140481695352640 builder_impl.py:460] No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/niki/samiAi/exported/saved_model/saved_model.pb\n",
      "I0716 16:12:49.446916 140481695352640 builder_impl.py:425] SavedModel written to: /home/niki/samiAi/exported/saved_model/saved_model.pb\n",
      "INFO:tensorflow:Writing pipeline config file to /home/niki/samiAi/exported/pipeline.config\n",
      "I0716 16:12:49.521279 140481695352640 config_util.py:254] Writing pipeline config file to /home/niki/samiAi/exported/pipeline.config\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "regex = re.compile(r\"model\\.ckpt-([0-9]+)\\.index\")\n",
    "numbers = [int(regex.search(f).group(1)) for f in os.listdir(OUTPUT_PATH) if regex.search(f)]\n",
    "TRAINED_CHECKPOINT_PREFIX = os.path.join(OUTPUT_PATH, 'model.ckpt-{}'.format(max(numbers)))\n",
    "\n",
    "print(f'Using {TRAINED_CHECKPOINT_PREFIX}')\n",
    "\n",
    "!rm -rf $EXPORTED_PATH\n",
    "!python -m object_detection.export_inference_graph \\\n",
    "  --pipeline_config_path=$DATA_PATH/pipeline.config \\\n",
    "  --trained_checkpoint_prefix=$TRAINED_CHECKPOINT_PREFIX \\\n",
    "  --output_directory=$EXPORTED_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NfIiNcb0OWk6"
   },
   "source": [
    "# Testing the model\n",
    "Let's test to see if our model is working. We only trained for 500 steps so the model's predictions might appear random or not even predict a box. Try taking a few photos. The results will look a lot better when we can try the model out on a real-time video stream later on.\n",
    "\n",
    "> **Homework:** Try training the model for 5,000 steps and see how the accuracy changes. Play around with other model formats, like the non-quantized version of SSD MobileNet V1 or Faster RCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWHvlnyjmCIN"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript, Image\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "\n",
    "# Use javascipt to take a photo.\n",
    "def take_photo(filename, quality=0.8):\n",
    "  js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const capture = document.createElement('button');\n",
    "      capture.textContent = 'Capture';\n",
    "      div.appendChild(capture);\n",
    "\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "  display(js)\n",
    "  data = eval_js('takePhoto({})'.format(quality))\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  with open(filename, 'wb') as f:\n",
    "    f.write(binary)\n",
    "  return filename\n",
    "\n",
    "try:\n",
    "  take_photo('/home/niki/samiAi/photo.jpg')\n",
    "except Exception as err:\n",
    "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
    "  # grant the page permission to access it.\n",
    "  print(str(err))\n",
    "\n",
    "# Use the captured photo to make predictions\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image as PImage\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "# Load the labels\n",
    "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP_PATH, use_display_name=True)\n",
    "\n",
    "# Load the model\n",
    "path_to_frozen_graph = os.path.join(EXPORTED_PATH, 'frozen_inference_graph.pb')\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(path_to_frozen_graph, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "with detection_graph.as_default():\n",
    "  with tf.Session(graph=detection_graph) as sess:\n",
    "    # Definite input and output Tensors for detection_graph\n",
    "    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "    # Each box represents a part of the image where a particular object was detected.\n",
    "    detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "    # Each score represent how level of confidence for each of the objects.\n",
    "    # Score is shown on the result image, together with the class label.\n",
    "    detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "    detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "    image = PImage.open('/home/niki/samiAi/photo.jpg')\n",
    "    # the array based representation of the image will be used later in order to prepare the\n",
    "    # result image with boxes and labels on it.\n",
    "    (im_width, im_height) = image.size\n",
    "    image_np = np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    # Actual detection.\n",
    "    (boxes, scores, classes, num) = sess.run(\n",
    "        [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "        feed_dict={image_tensor: image_np_expanded})\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        np.squeeze(boxes),\n",
    "        np.squeeze(classes).astype(np.int32),\n",
    "        np.squeeze(scores),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=8)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVmCc9NPkP3U"
   },
   "source": [
    "# Convert the model\n",
    "Once we have have the TensorFlow graph proto we can use it in Python, but we are going to take it a step further and convert the model to TensorFlow.js so we can use it directly in the browser.\n",
    "\n",
    "The model only detects objects as the IDs in out `label_map.pbtxt` so we also need to create a json list of all of our labels so we can map the ID back to a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-f5lfcnp01e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-16 16:16:35.809549: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\n",
      "2020-07-16 16:16:35.809862: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   debug_stripper: Graph size after: 3114 nodes (0), 3745 edges (0), time = 22.353ms.\n",
      "2020-07-16 16:16:35.809911: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   model_pruner: Graph size after: 2100 nodes (-1014), 2355 edges (-1390), time = 93.117ms.\n",
      "2020-07-16 16:16:35.809956: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 625 nodes (-1475), 687 edges (-1668), time = 1178.13ms.\n",
      "2020-07-16 16:16:35.810000: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 385 nodes (-240), 646 edges (-41), time = 95.89ms.\n",
      "2020-07-16 16:16:35.810045: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 330 nodes (-55), 557 edges (-89), time = 20.125ms.\n",
      "2020-07-16 16:16:35.810089: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   model_pruner: Graph size after: 330 nodes (0), 557 edges (0), time = 16.354ms.\n",
      "2020-07-16 16:16:35.810133: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 330 nodes (0), 557 edges (0), time = 112.466ms.\n",
      "2020-07-16 16:16:35.810177: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 328 nodes (-2), 553 edges (-4), time = 67.859ms.\n",
      "2020-07-16 16:16:35.810221: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 328 nodes (0), 553 edges (0), time = 16.512ms.\n",
      "2020-07-16 16:16:35.810265: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   debug_stripper: debug_stripper did nothing. time = 1.157ms.\n",
      "2020-07-16 16:16:35.810309: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   model_pruner: Graph size after: 328 nodes (0), 553 edges (0), time = 15.33ms.\n",
      "2020-07-16 16:16:35.810353: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 328 nodes (0), 553 edges (0), time = 58.749ms.\n",
      "2020-07-16 16:16:35.810397: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 328 nodes (0), 553 edges (0), time = 67.738ms.\n",
      "2020-07-16 16:16:35.810680: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 328 nodes (0), 553 edges (0), time = 16.954ms.\n",
      "2020-07-16 16:16:35.810726: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   model_pruner: Graph size after: 328 nodes (0), 553 edges (0), time = 16.57ms.\n",
      "2020-07-16 16:16:35.810770: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 328 nodes (0), 553 edges (0), time = 63.686ms.\n",
      "2020-07-16 16:16:35.810815: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 328 nodes (0), 553 edges (0), time = 70.069ms.\n",
      "2020-07-16 16:16:35.810859: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 328 nodes (0), 553 edges (0), time = 18.2ms.\n",
      "2020-07-16 16:16:42.391049: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\n",
      "2020-07-16 16:16:42.391313: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   remapper: Graph size after: 322 nodes (-6), 547 edges (-6), time = 70.276ms.\n",
      "2020-07-16 16:16:42.391375: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 322 nodes (0), 547 edges (0), time = 140.799ms.\n",
      "2020-07-16 16:16:42.391434: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 322 nodes (0), 547 edges (0), time = 112.107ms.\n",
      "2020-07-16 16:16:42.391492: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 322 nodes (0), 547 edges (0), time = 34.572ms.\n",
      "2020-07-16 16:16:42.391554: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   remapper: Graph size after: 322 nodes (0), 547 edges (0), time = 25.784ms.\n",
      "2020-07-16 16:16:42.393091: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 322 nodes (0), 547 edges (0), time = 109.45ms.\n",
      "2020-07-16 16:16:42.393375: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 322 nodes (0), 547 edges (0), time = 122.521ms.\n",
      "2020-07-16 16:16:42.393479: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 322 nodes (0), 547 edges (0), time = 33.247ms.\n",
      "Writing weight file /home/niki/samiAi/model_web/model.json...\n",
      "  adding: group1-shard1of2.bin (deflated 96%)\n",
      "  adding: group1-shard2of2.bin (deflated 96%)\n",
      "  adding: labels.json (stored 0%)\n",
      "  adding: model.json (deflated 94%)\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter \\\n",
    "  --input_format=tf_frozen_model \\\n",
    "  --output_format=tfjs_graph_model \\\n",
    "  --output_node_names='Postprocessor/ExpandDims_1,Postprocessor/Slice' \\\n",
    "  --quantization_bytes=1 \\\n",
    "  --skip_op_check \\\n",
    "  $EXPORTED_PATH/frozen_inference_graph.pb \\\n",
    "  /home/niki/samiAi/model_web\n",
    "\n",
    "import json\n",
    "\n",
    "from object_detection.utils.label_map_util import get_label_map_dict\n",
    "\n",
    "label_map = get_label_map_dict(LABEL_MAP_PATH)\n",
    "label_array = [k for k in sorted(label_map, key=label_map.get)]\n",
    "\n",
    "with open(os.path.join('/home/niki/samiAi/model_web', 'labels.json'), 'w') as f:\n",
    "  json.dump(label_array, f)\n",
    "\n",
    "!cd /home/niki/samiAi/model_web && zip -r /home/niki/samiAi/model_web.zip *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1iFI9pPr1l7"
   },
   "source": [
    "# Download the model\n",
    "> **Note:** Sometimes this command doesn't run or it will throw an error. Just try running it again.\n",
    "\n",
    "You can also download the model by right clicking on the `model_web.zip` file in the left sidebar file inspector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FL_miSj2r1yt"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('/home/niki/samiAi/model_web.zip') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P3WHwIZ0QlVP"
   },
   "source": [
    "# Next steps / Using the model\n",
    "Use the `model_web` folder generated here, with this [project](https://github.com/cloud-annotations/object-detection-react) to create a real-time webcam object detection app."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "object-detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
