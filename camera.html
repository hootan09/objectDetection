<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1, shrink-to-fit=no, user-scalable=no"
    />
    <title>Object Detection Stream</title>
  </head>
  <body>
    <!-- <script src="https://cdn.jsdelivr.net/npm/@cloud-annotations/models@0.1.1"></script> -->
    <script src="/models@0.1.1"></script>
    <button onclick="useRearCamera()">Use Rear Camera</button>
    <br>
    <video
      id="video"
      width="600"
      height="400"
      style="position: fixed;"
      muted
      controls
    ></video>
    
    <img id="loading" width="600" height="400" style="position: fixed;" src="loading.gif" alt="loading">

    <canvas
      id="canvas"
      width="600"
      height="400"
      style="position: fixed;"
    ></canvas>
    <script>
      const video = document.getElementById("video");
      const canvas = document.getElementById("canvas");

    // Get access to the camera!
    if(navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
        // Not adding `{ audio: true }` since we only want video now
        navigator.mediaDevices.getUserMedia({ video: true }).then(function(stream) {
            //video.src = window.URL.createObjectURL(stream);
            console.log(stream);
            video.srcObject = stream;
            video.play();
        });
    }

    function useRearCamera(){
        if(navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
        // Not adding `{ audio: true }` since we only want video now
        navigator.mediaDevices.getUserMedia({ video: { facingMode: { exact: "environment" }} }).then(function(stream) {
            //video.src = window.URL.createObjectURL(stream);
            console.log(stream);
            video.srcObject = stream;
            video.play();
        });
    }
    }

      video.addEventListener("loadeddata", function () {
        // Model from: https://github.com/tensorflow/tfjs-models/tree/master/coco-ssd
        models.load("/model_web").then((model) => {
          document.getElementById("loading").style.visibility = "hidden";
          detectFrame(model)
          });
      });

      const detectFrame = async (model) => {
        const predictions = await model.detect(video);
        renderPredictions(predictions);
        requestAnimationFrame(() => {
          detectFrame(model);
        });
      };

      const renderPredictions = (predictions) => {
        const ctx = canvas.getContext("2d");
        ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);
        // Font options.
        const font = "16px sans-serif";
        ctx.font = font;
        ctx.textBaseline = "top";
        predictions.forEach((prediction) => {
          const x = prediction.bbox[0];
          const y = prediction.bbox[1];
          const width = prediction.bbox[2];
          const height = prediction.bbox[3];
          const label = `${prediction.class}: ${prediction.score.toFixed(2)}`;
          // Draw the bounding box.
          ctx.strokeStyle = "#FFFF3F";
          ctx.lineWidth = 5;
          ctx.strokeRect(x, y, width, height);
          // Draw the label background.
          ctx.fillStyle = "#FFFF3F";
          const textWidth = ctx.measureText(label).width;
          const textHeight = parseInt(font, 10); // base 10
          ctx.fillRect(x, y, textWidth + 4, textHeight + 4);
        });

        predictions.forEach((prediction) => {
          const x = prediction.bbox[0];
          const y = prediction.bbox[1];
          const label = `${prediction.class}: ${prediction.score.toFixed(2)}`;
          // Draw the text last to ensure it's on top.
          ctx.fillStyle = "#000000";
          ctx.fillText(label, x, y);
        });
      };

    </script>
  </body>
</html>